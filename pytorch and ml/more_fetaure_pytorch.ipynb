{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6820ce7c",
   "metadata": {},
   "source": [
    "<h2>autograd and training </h2>\n",
    "<h3>in pytorch</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b072881b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m w = torch.tensor(\u001b[32m4.0\u001b[39m,requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m x = torch.tensor(\u001b[32m5.0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m w = torch.tensor(\u001b[32m4.0\u001b[39m,requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m x = torch.tensor(\u001b[32m5.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1112\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1090\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2188\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2185\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         keep_suspended = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2193\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2257\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2254\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2255\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m         \u001b[43mnotify_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2258\u001b[39m         notify_event.clear()\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:629\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    627\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\threading.py:331\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    333\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor(4.0,requires_grad=True)\n",
    "x = torch.tensor(5.0)\n",
    "y = w * x\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f382ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.tensor(2.0)\n",
    "w = torch.tensor(5.0, requires_grad=True)\n",
    "y = x * w\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e079f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor(2.0)\n",
    "w = torch.tensor(6.0, requires_grad=True)\n",
    "yp = w * x\n",
    "yt = torch.tensor(10.0)\n",
    "loss = (yp - yt) **2\n",
    "print(loss)\n",
    "loss.backward()\n",
    "print(\"loss backward \", w.grad)\n",
    "\n",
    "print(\"pediction\", yp.item())\n",
    "print(\"loss\", loss.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df897f97",
   "metadata": {},
   "source": [
    "# Gradient Descent — Math, Explanation & Code\n",
    "\n",
    "---\n",
    "\n",
    "## The Core Idea\n",
    "\n",
    "Gradient Descent answers one question:\n",
    "\n",
    "> *\"How do I adjust `w` to make my loss smaller?\"*\n",
    "\n",
    "---\n",
    "\n",
    "## The Math Notation\n",
    "\n",
    "The update rule is:\n",
    "\n",
    "$$w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "Where:\n",
    "- $w$ = weight (what we're learning)\n",
    "- $\\alpha$ = **learning rate** (how big each step is)\n",
    "- $\\frac{\\partial L}{\\partial w}$ = **gradient** (which direction is \"downhill\")\n",
    "\n",
    "---\n",
    "\n",
    "## Deriving the Gradient by Hand\n",
    "\n",
    "Our loss function is:\n",
    "\n",
    "$$L = (yp - yt)^2 = (w \\cdot x - yt)^2$$\n",
    "\n",
    "To find the gradient, we differentiate with respect to $w$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = 2(w \\cdot x - yt) \\cdot x$$\n",
    "\n",
    "Plugging in our values $w=3, x=2, yt=10$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = 2(3 \\cdot 2 - 10) \\cdot 2 = 2(-4) \\cdot 2 = -16$$\n",
    "\n",
    "The **negative** value means → *\"increase `w` to reduce loss\"*\n",
    "\n",
    "\n",
    " ### this gradient will minizie the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f167ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-16.)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "x = torch.tensor(2.0)\n",
    "yt = torch.tensor(10)\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "loss = (w * x - yt)**2\n",
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd910da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor(2.0)\n",
    "w = torch.tensor(4.0, requires_grad=True)\n",
    "yt = torch.tensor(15.0)\n",
    "lr = 0.1\n",
    "yp = w * x\n",
    "loss = (yp - yt)**2\n",
    "loss.backward()\n",
    "print(\"before update \")\n",
    "print(w.item())\n",
    "print(loss.item())\n",
    "\n",
    "\n",
    "print(f\"New weight after update: {w.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ba51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x  = torch.tensor(2.0)\n",
    "w  = torch.tensor(4.0, requires_grad=True)\n",
    "yt = torch.tensor(15.0)\n",
    "lr = 0.1\n",
    "yp = w * x\n",
    "loss = (yp - yt)**2\n",
    "loss.backward()\n",
    "print(\"before update\")\n",
    "print(\"w:\" , w.item())\n",
    "print(\"loss :\", loss.item())\n",
    "\n",
    "print(\"loss:\" , loss.item())\n",
    "print(\"gradient:\" , w.grad.item())\n",
    "\n",
    "#update weight\n",
    "with torch.no_grad():\n",
    "    w -= lr * w.grad\n",
    "w.grad.zero_()\n",
    "print(\"after update\")\n",
    "print(\"W:\" , w.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
