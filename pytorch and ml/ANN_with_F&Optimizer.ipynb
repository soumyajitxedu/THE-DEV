{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b2561d",
   "metadata": {},
   "source": [
    "<h3>day 4 with ann and function and optimizer</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1630b40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1582,  2.6121], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "layer = nn.Linear(3, 2)  # input features = 3, output features = 2\n",
    "x = torch.tensor([1.0,2.0,3.0])\n",
    "y = layer(x)\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac2f578",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'module'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmymodel\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodule\u001b[49m):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m      8\u001b[39m         \u001b[38;5;28msuper\u001b[39m.\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch.nn' has no attribute 'module'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "#model\n",
    "class mymodel(nn.module):\n",
    "    def __init__(self):\n",
    "        super.__init__()\n",
    "        self.linear == nn.Linear(1,1)\n",
    "    def forward(self,x):\n",
    "        return self.Linear(x)\n",
    "    \n",
    "    x = torch.tensor([[1.],[2.],[3.],[4.]])\n",
    "    y = torch.tensor([[2.],[4.],[6.],[8.]])\n",
    "model = mymodel()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimiser = optim.Adam(model.parameters(),lr = 0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimiser.zero_grad()\n",
    "    yp = model(x)\n",
    "    loss = loss_fn(yp,y)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "print(f\"epoch{epoch + 1},loss : {loss.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86460b84",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x3 and 4x8)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     yp   = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     loss = criterion(yp, yt)\n\u001b[32m     26\u001b[39m     optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\maina\\Downloads\\soumyajit drive\\pythonlst\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\maina\\Downloads\\soumyajit drive\\pythonlst\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mMyNetwork.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)     \u001b[38;5;66;03m# hidden â†’ ReLU\u001b[39;00m\n\u001b[32m     15\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28mself\u001b[39m.layer2(x))     \u001b[38;5;66;03m# hidden â†’ ReLU\u001b[39;00m\n\u001b[32m     16\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.softmax(\u001b[38;5;28mself\u001b[39m.layer3(x))  \u001b[38;5;66;03m# output â†’ Softmax\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\maina\\Downloads\\soumyajit drive\\pythonlst\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\maina\\Downloads\\soumyajit drive\\pythonlst\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\maina\\Downloads\\soumyajit drive\\pythonlst\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (1x3 and 4x8)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1  = nn.Linear(4, 8)\n",
    "        self.layer2  = nn.Linear(8, 4)\n",
    "        self.layer3  = nn.Linear(4, 3)\n",
    "        self.relu    = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))     # hidden â†’ ReLU\n",
    "        x = self.relu(self.layer2(x))     # hidden â†’ ReLU\n",
    "        x = self.softmax(self.layer3(x))  # output â†’ Softmax\n",
    "        return x\n",
    "\n",
    "model     = MyNetwork()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    yp   = model(x)\n",
    "    loss = criterion(yp, yt)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a38a6a7",
   "metadata": {},
   "source": [
    "# Day 4 Complete Recap ðŸ““\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Why Activation Functions?\n",
    "\n",
    "Without them:\n",
    "\n",
    "$$y = w_2(w_1x + b_1) + b_2 = \\text{still a straight line!}$$\n",
    "\n",
    "With them:\n",
    "```\n",
    "Network can learn ANY shape! âœ…\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ReLU\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$f(x) = max(0, x)$$\n",
    "\n",
    "**Behavior:**\n",
    "```\n",
    "negative â†’ 0\n",
    "positive â†’ keep as is\n",
    "```\n",
    "\n",
    "**Derivative:**\n",
    "```\n",
    "x > 0 â†’ 1 (gradient flows!)\n",
    "x < 0 â†’ 0 (gradient stops!)\n",
    "```\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "self.relu = nn.ReLU()\n",
    "x = self.relu(x)\n",
    "```\n",
    "\n",
    "**Use:** Hidden layers always! âœ…\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Sigmoid\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "**Behavior:**\n",
    "```\n",
    "any number â†’ squeezes to 0-1\n",
    "output = probability!\n",
    "```\n",
    "\n",
    "**Derivative:**\n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$$\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "self.sigmoid = nn.Sigmoid()\n",
    "x = self.sigmoid(x)\n",
    "```\n",
    "\n",
    "**Use:** Binary output (yes/no) âœ…\n",
    "\n",
    "**Problem:** Vanishing gradient! âŒ\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Softmax\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$\n",
    "\n",
    "**Behavior:**\n",
    "```\n",
    "list of numbers â†’ probabilities\n",
    "all outputs sum to 1.0 always!\n",
    "```\n",
    "\n",
    "**Simple way to think:**\n",
    "```\n",
    "just fancy percentage calculator!\n",
    "uses e^x to make winner more obvious\n",
    "```\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "self.softmax = nn.Softmax(dim=1)\n",
    "x = self.softmax(x)\n",
    "```\n",
    "\n",
    "**Use:** Multi-class output âœ…\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Tanh\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "**Behavior:**\n",
    "```\n",
    "any number â†’ squeezes to -1 to 1\n",
    "```\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "self.tanh = nn.Tanh()\n",
    "x = self.tanh(x)\n",
    "```\n",
    "\n",
    "**Use:** RNNs and some hidden layers âœ…\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Activation Functions Summary\n",
    "\n",
    "```\n",
    "Function   Formula        Range      Use where?\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ReLU       max(0,x)       0 to âˆž     Hidden âœ…\n",
    "Sigmoid    1/(1+e^-x)     0 to 1     Binary out\n",
    "Softmax    e^x/Î£e^x       0 to 1     Multi out\n",
    "Tanh       ex-e-x/ex+e-x  -1 to 1    RNNs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. SGD Optimizer\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "**With Momentum:**\n",
    "\n",
    "$$v = \\beta \\cdot v + \\alpha \\cdot \\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "$$w = w - v$$\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "# Simple\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# With momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                             lr=0.01, \n",
    "                             momentum=0.9)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Adam Optimizer\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$m_t = \\beta_1 \\cdot m_{t-1} + (1-\\beta_1) \\cdot g_t$$\n",
    "\n",
    "$$v_t = \\beta_2 \\cdot v_{t-1} + (1-\\beta_2) \\cdot g_t^2$$\n",
    "\n",
    "$$w = w - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t$$\n",
    "\n",
    "**Simple version:**\n",
    "```\n",
    "adjusts lr per weight automatically\n",
    "fast + smooth + works without tuning!\n",
    "```\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Optimizers Summary\n",
    "\n",
    "```\n",
    "Optimizer   lr       Best for\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SGD         0.01     Simple problems\n",
    "SGD+mom     0.01     Better SGD\n",
    "Adam        0.001    Everything! âœ…\n",
    "RMSProp     0.01     RNNs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Complete Network With Everything\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1  = nn.Linear(4, 8)\n",
    "        self.layer2  = nn.Linear(8, 4)\n",
    "        self.layer3  = nn.Linear(4, 3)\n",
    "        self.relu    = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))     # hidden â†’ ReLU\n",
    "        x = self.relu(self.layer2(x))     # hidden â†’ ReLU\n",
    "        x = self.softmax(self.layer3(x))  # output â†’ Softmax\n",
    "        return x\n",
    "\n",
    "model     = MyNetwork()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    yp   = model(x)\n",
    "    loss = criterion(yp, yt)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 11. The Golden Rules ðŸ“Œ\n",
    "\n",
    "```\n",
    "Hidden layers  â†’ ReLU (always!)\n",
    "Binary output  â†’ Sigmoid\n",
    "Multi output   â†’ Softmax\n",
    "\n",
    "Just starting? â†’ Adam (always!)\n",
    "lr for SGD     â†’ 0.01\n",
    "lr for Adam    â†’ 0.001 (10x smaller!)\n",
    "\n",
    "Training order NEVER changes:\n",
    "zero_grad() â†’ backward() â†’ step()\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
